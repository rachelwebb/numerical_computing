\lab{Application}{Facial Recognition using Eigenfaces}{Facial Recognition using Eigenfaces}
\label{lab:FacialRecognition}

\objective{Use the singular value decomposition to build a facial recognition system.}

Suppose we have a large database containing images of human faces.
We would like to identify people by matching their pictures to those in the database.
This task is called \emph{facial recognition}.

Facial recognition is important in law enforcement, as well as other situations. 
For example, facial recognition can be combined with video surveillance to identify when a person is not authorized to be in a location.

Humans can easily compare two face images and determine whether they belong to the same person, but automating this process is more challenging.
One technique for automated facial recognition uses \emph{eigenfaces}.

\section*{Load the data}
Eigenfaces are an efficient way to store and query a database of face images.
As the name suggests, this method uses eigenvectors of matrices related to the collection of face images.
Essentially, the method of eigenfaces projects face images to a lower-dimensional subspace in a way that preserves their distinguishing characteristics. 
In the lower-dimensional subspace, comparing face images is much faster.

Recall that a digital image may be stored as an $m \times n$ array of pixel intensities. 
In this lab, we will store the images as $mn$-vectors by concatenating the rows of the $m \times n$ arrays.


\begin{problem}
\label{prob:getTrainingFaces}
Download the \li{faces94} face image database found at \url{http://cswww.essex.ac.uk/mv/allfaces/faces94.html} and extract the files.
You should now have a directory named ``faces94", which contains multiple face images each of a large collection of individuals.
We will construct a database of face images by selecting exactly one face image for each person in the directory.
Execute the following code to construct the collection of faces (you may need to change the variable \li{path} to point to the location
of the \li{faces94} directory on your machine):
\begin{lstlisting}
>>> import numpy as np
>>> from os import walk
>>> from scipy.ndimage import imread
>>> # these are the dimensions of the images
>>> w = 200
>>> h = 180
>>> # traverse the directory, get one image per subdirectory
>>> path = "./faces94"
>>> faces = []
>>> for (dirpath, dirnames, filenames) in walk(path):
>>>     for f in filenames:
>>>         if f[-3:]=="jpg": # only get jpg images
>>>             # load image, convert to grayscale, flatten into vector
>>>             face = imread(dirpath+"/"+f).mean(axis=2).ravel()
>>>             faces.append(face)
>>>             break
>>> # put all the face vectors column-wise into a matrix
>>> F = np.array(faces).T
\end{lstlisting}
The array \li{F} should be a $360000 \times 153$ matrix whose columns consist of 153 face images.
\end{problem}


\section*{Shift by the mean}

We can make our algorithm more robust by using a trick called \emph{shifting by the mean}. 
Suppose we have a collection of $k$ face images represented as vectors $f_1, f_2, \ldots, f_k$ of length $mn$.
Define the \emph{mean face} $\mu$ to be the average of the $f_i$:
\[
\mu = \frac{1}{k}\sum_{i=1}^k f_i.
\]
\begin{figure}
\includegraphics[width=0.3\textwidth]{meanFace.png}
\caption{The mean face.}
\label{facialRecognition:meanFace}
\end{figure}

\begin{problem}
\label{prob:meanFace}
Calculate the mean face image \li{mu} from the face image array \li{F} that you obtained in problem \ref{prob:getTrainingFaces}.
This can be done easily using \li{np.mean} and specifying the correct axis.

Plot the mean face.
As a convenience, you may use the following function to plot the image given just the flat vector representation:
\begin{lstlisting}
>>> from matplotlib import pyplot as plt
>>> import matplotlib.cm as cm
>>> def show(im, w=200, h=180):
>>>     """
>>>     Plot the w by h image whose pixel values are given by the array im.
>>>     """
>>>     plt.imshow(im.reshape((w,h)), cmap=cm.Greys_r)
>>>     plt.show()
>>> # given that you have calculated the mean face mu, show it
>>> show(mu)
\end{lstlisting}
Your mean face should match that shown in Figure \ref{facialRecognition:meanFace}.
\end{problem}
\begin{figure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{differenceFace0.png}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{differenceFace1.png}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{differenceFace2.png}
\end{subfigure}
\caption{Three mean-shifted faces from the dataset.}
\label{facialRecognition:differenceFaces}
\end{figure}
For each $i = 1,\ldots, k$, define $\bar{f}_i := f_i - \mu$.
The mean-shifted face vector $\bar{f}_i$ is the deviation of the $i$-th face from the mean, and thus captures the unique features of the face.
Now form the $mn \times k$ matrix $\bar{F}$ whose columns are given by the mean-shifted face vectors, i.e.
\[
\bar{F} = \begin{bmatrix}
\bar{f}_1 & \bar{f}_2 & \cdots & \bar{f}_k
\end{bmatrix}.
\]
\begin{problem}
Using the face matrix \li{F} and the mean face \li{mu} from problems \ref{prob:getTrainingFaces} and \ref{prob:meanFace}, calculate the
matrix of mean-shifted face vectors \li{A}.
(This can be done in a nice, vectorized manner using array broadcasting.)
Plot the first mean-shifted face, i.e. the first column of the matrix \li{A}.
It should match the first face in Figure \ref{facialRecognition:differenceFaces}.
\end{problem}

\section*{Project to the face space}
Now suppose we have a new face vector $g$. 
How can we find the closest face image in our database?
A naive approach is to subtract the mean face vector from $g$, obtaining $\bar{g} = g-\mu$, and then compute the Euclidean distance from $\bar{g}$ to each mean-shifted face vector $\bar{f}_i$.
Then we can find the index $i$ where the distance $\|\bar{g}-\bar{f}_i\|_2$ is smallest, and return the $i$-th face image as the best match.
Unfortunately, this approach is computationally intractable when the length $mn$ of the vectors $f_i$ is large.
Since a low-resolution photo may easily have $100\times 100 = 10,000$ pixels, in practice $mn$ is very large indeed.

The trick is to reduce the dimension of the face vectors $f_i$, so that instead of living in the $mn$-dimensional space of all possible images, they live in the subspace of face images.
We call this subspace the \emph{face space}.
We may approximate the face space as the span of the face images in our database; i.e., as the span of the columns of $\bar{F}$, or equivalently the range $\mathscr{R}(\bar{F})$ of $\bar{F}$.

To efficiently compare $\bar{g}$ to the database faces $\bar{f}_i$, we need to project $\bar{g}$ into the face space. In fact,
\[
proj_{\mathscr{R}(\bar{f})} \bar{g} = \overline{F}\bar{F}^{\dagger}\bar{g},
\]
where $\bar{F}^{\dagger}$ is the Moore-Penrose pseudoinverse of $\bar{F}$ [cite Proposition 4.5.3 of the text].
 Call this projection matrix $P$. 
 Furthermore, if $U_1\Sigma_1V_1^T$ is the compact form of the SVD of $\bar{F}$, then $\bar{F}^{\dagger} = V_1\Sigma_1^{-1}U_1^T$ [cite Theorem 4.5.1 of the text]. 
 Thus
\begin{align*}
P = proj_{\mathscr{R}(\bar{F})} &= U \Sigma V^T V_1\Sigma_1^{-1}U_1^T\\
&= \begin{pmatrix}
U_1 & U_2
\end{pmatrix} \begin{pmatrix}
\Sigma_1 & 0\\
0 & 0 
\end{pmatrix} \begin{pmatrix}
V_1^T \\
V_2^T
\end{pmatrix} V_1\Sigma_1^{-1}U_1^T\\
&= \begin{pmatrix}
U_1\Sigma_1 & 0
\end{pmatrix} \begin{pmatrix}
I \\
0
\end{pmatrix}\Sigma_1^{-1}U_1^T\\
&= U_1U_1^T.
\end{align*}
Because the columns of $U_1$ are eigenvectors of $AA^T$, we call them \emph{eigenfaces}.
Our computation shows that the eigenfaces are an orthonormal basis for the face space.

\begin{problem}
\label{prob:svd}
Calculate the singular value decomposition of the matrix \li{A} as follows:
\begin{lstlisting}
>>> from scipy imort linalg as la
>>> U,Sig,Vh = la.svd(A, full_matrices=False)
\end{lstlisting}
The keyword argument \li{full_matrices=False} cause the function to only return the columns of $U$ and $V$ corresponding
to positive singular values. Hence, \li{U} should be a $36000\times 153$ matrix whose columns are the eigenfaces.
Plot the first eigenface (i.e. the first column of \li{U}).
It should match the first eigenface shown in Figure \ref{facialRecognition:eigenfaces}.
By plotting some of the other eigenfaces, note that each has face-like properties, and different eigenfaces tend to capture different
types of facial structures.
\end{problem}

\section*{Project to a subspace of the face space}
Unfortunately, the face space still has too many dimensions to be computationally efficient.
We wish to further reduce the dimension of the space we're working in by projecting to an $s$-dimensional subspace of the face space.

By the Schmidt-Eckart-Young-Mirsky theorem from Lab [cite prev. lab], the best rank-$s$ approximation to $P$ can be computed from its compact SVD.
But we already know a compact SVD of $P$, since $P = \bar{F}\bar{F}^{\dagger} = U_1 I_s U_1^T$. So by this theorem, the best rank-$s$ approximation to $P$ is $\widehat{P}_s = U_sU_s^T$, where $U_s$ is the first $s$ columns of $U_1$.

Therefore, the $s$-dimensional subspace that best approximates the face space is spanned by the first $s$ eigenfaces. 
We may project any vector into this subspace with the matrix $\widehat{P}_s = U_sU_s^T$.

However, it does us no good to project all our vectors into an $s$-dimensional space if we still store them as vectors in $\mathbb{R}^{nm}$. 
Thus, we must store our face vectors in terms of a basis for this subspace---that is, we must change our basis to the columns of $U_s$.
The change-of-basis matrix is $U_s^T$, so $U_s^T\widehat{P}_s = U_s^TU_sU_s^T = U_s^T$. 
Thus we can project into the subspace and change basis by multiplying by $U_s^T$.
To change back to the full $mn$-vector, multiply by $U_s$.

\begin{comment}
The discussion in the paragraph below needs some work.
I need to better understand why the eigenfaces basis works so well.

Although this approach is quite sensible, there are a few issues.
Firstly, as the number of face images in our database grows large, both storing the full matrix $A$ and computing $\|\bar{g}-\bar{f}_i\|_2$ for all $i$
become increasingly difficult from a computational standpoint.
Secondly -- and this is the more important point -- calculating the Euclidean distance between two face images in the standard basis may not be the best measure
of how similar the two faces are (and empirical evidence supports this).
For example, if we have two images of the same face but with different backgrounds, the Euclidean distance between the face vectors may be very large,
since in the standard basis, every pixel is weighted equally.
Given these issues, it is natural to ask ourselves whether there exists a low-dimensional subspace with a basis in which we can represent our
face images with minimal error and where the Euclidean distance is a more robust measure of face similarity.
\end{comment}
\begin{comment}
Since the face images all have much in common, we have reason to hope that they all lie (nearly) in a low-dimensional subspace of $\mathbb{R}^l$.
The singular value decomposition of $A$ gives us just such a subspace.
In particular, if we write the singular value decomposition as
\[
A = U\Sigma V^H,
\]
then the columns of $U$ corresponding to the positive singular values form an orthonormal basis for the columns of $A$.
Recall that there are $r$ positive singular values, where $r$ is the rank of $A$.
Hence, if $r \ll l$, then we have found a low-dimensional subspace in which to represent our face vectors.
We call this subspace the \emph{eigenface subspace}, and we call each basis vector (i.e. each column of $U$ corresponding
to a positive singular value) an \emph{eigenface}.
\end{comment}


\begin{figure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{eigenface0.png}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{eigenface1.png}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{eigenface2.png}
\end{subfigure}
\caption{The top three eigenfaces.}
\label{facialRecognition:eigenfaces}
\end{figure}


\begin{comment}
Not all of the eigenfaces are equally important, however.
In particular, the eigenfaces corresponding to the singular values closest to 0 contribute the least, on average, to the face vectors in $A$.
Thus, we can get away with only using the eigenfaces corresponding to the $n$ largest singular values (call these the top $n$ eigenfaces).
This is important when the rank $r$ is still too large to be practical.
Define $U_n$ to be the $l \times n$ matrix whose columns consist of the top $n$ eigenfaces.
The the coordinate vector of $\bar{f}_i$ in terms of these top $n$ eigenfaces is given by
\[
\hat{f}_i := U_n^H\bar{f}_i.
\]
Hence the matrix $\hat{A}$ defined by
\[
\hat{A}_n = \begin{bmatrix}
\hat{f}_1 & \hat{f}_2 & \ldots & \hat{f}_k
\end{bmatrix}
= U_n^HA
\]
contains as its columns the coordinate vectors of the face images in the top $n$ eigenfaces basis.
Note that $\hat{A}$ is an $n \times k$ matrix, and is therefore much smaller than the original $l \times k$ matrix $A$.
\end{comment}


\begin{problem}
\label{prob:top_n}
The columns of the matrix \li{U} that you computed in problem \ref{prob:svd} are already ordered according to descending singular values.
Thus, the first $n$ columns of \li{U} give the top $n$ eigenfaces.
Write a function \li{nEigenfaces} that takes as input \li{U}, \li{A}, and an integer $n$, and returns the matrices $U_n$ and $\hat{A}_n$ as described above.
\end{problem}
\begin{figure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{rebuiltAll.png}
\caption{All of the eigenfaces.}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{rebuiltHalf.png}
\caption{1/2 of the eigenfaces.}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{rebuiltFourth.png}
\caption{1/4 of the eigenfaces.}
\end{subfigure}
\caption{Image rebuilt with varying number of eigenfaces}
\label{facialRecognition:rebuiltImage}
\end{figure}

\begin{comment}
Of course, moving to a lower-dimensional subspace by using only the top $n$ eigenfaces introduces some error, in the sense that we cannot perfectly
reconstruct the face images from their coordinate vectors.
However, this error is tolerable, provided $n$ is not \emph{too} small.
Note that we can approximately reconstruct $\bar{f}_i$ from the coordinate vector $\hat{f}_i$ by
\[
\bar{f}_i \approx U_n\hat{f}_i.
\]
See Figure \ref{facialRecognition:rebuiltImage} for examples of reconstructed face images giving different numbers of eigenfaces.
\end{comment}

\section*{Recognizing faces}
Finally, we are ready to identify which mean-shifted image $\bar{f}_i$ is closest to $\bar{g}$. 
We begin by projecting all vectors to some $s$-dimensional subspace and writing them in terms of an orthonormal basis for that subspace. 
This is accomplished with multiplication by $U_s^T$:
\[
\widehat{f}_i = U_s^T(f_i-\mu) \qquad \widehat{g} = U_s^T(\widehat{g}-\mu).
\]

Next, we compute which $\widehat{f}_i$ is closest to $\widehat{g}$. 
Since the columns of $U_s$ are an orthonormal basis, we get the same result doing the computation in this basis as we would in the standard Euclidean basis.
Define
\[
i^* = \text{argmin}_i \|\widehat{f}_i - \widehat{g}\|_2.
\]
Then the $i^*$-th face image is the best match for $g$.

\begin{comment}
We now have a low-dimensional representation of our database of face images given by $\hat{A}_n$, an array of eigenfaces given by $U_n$,
and the mean face $\mu$. Using just these objects, we can now perform facial recognition.
Let $g$ be a face vector.
We seek to find the face vector $f_i$ in the database that is closest to $g$.
In order to do so, we must process $g$ just as we processed each $f_i$, namely by subtracting out the mean face and then projecting onto the
eigenfaces basis.
Doing so yields the low-dimensional representation $\hat{g}$, given by
\[
\hat{g} = U_n^H(g-\mu).
\]
We then return the matching face image $f_{i^*}$, where
\[
i^* = \text{argmin}_i \|\hat{f}_i - \hat{g}\|_2.
\]
\end{comment}


\begin{problem}
\label{prob:nearest}
Write a function \li{findNearest} that accepts a matrix $M$ of shape $a \times b$ and a vector $x$
of length $a$, and returns the index of the column of $M$ that is closest to $x$ with respect to Euclidean distance.
Using the NumPy functions \li{np.linalg.norm} and \li{np.argmin} will be useful here.
\end{problem}

\begin{problem}
\label{prob:match}
We will perform facial recognition on additional images from the \li{faces94} dataset.


Use the following code to build an array of additional images from the dataset:
\begin{lstlisting}
>>> # let's gather some random images from the directory, and try to recognize them!
>>> n_tests = 10
>>> test_files = []
>>> for (dirpath, dirnames, filenames) in walk(path):
>>>     for f in filenames:
>>>         if f[-3:]=="jpg": # only get jpg images
>>>             test_files.append(dirpath+"/"+f)
>>> test_files = sample(test_files, n_tests)
>>> test_images = np.array([imread(f).mean(axis=2).ravel() for f in test_files]).T
\end{lstlisting}
You can change the variable \li{n_tests} to increase or decrease the number of face images to test.

Using your function \li{findNearest} from problem \ref{prob:nearest}, loop through the test images and find
the index of best matching image from the database.
As you do so, plot the test face alongside the matching face from the database to determine whether your system
really is properly recognizing the faces.
Use the following code to conveniently plot two face vectors side-by-side:
\begin{lstlisting}
>>> def show2(im1, im2, w=200, h=180):
>>>     """Plot face vectors im1 and im2 side-by-side."""
>>>     plt.subplot(121)
>>>     plt.imshow(im1.reshape((w,h)), cmap=cm.Greys_r)
>>>     plt.subplot(122)
>>>     plt.imshow(im2.reshape((w,h)), cmap=cm.Greys_r)
>>>     plt.show()
\end{lstlisting}

Repeat this experiment for different values of $n$, the number of eigenfaces to include.
The system should improve as $n$ gets larger.
\end{problem}

By this point, you have created a basic facial recognition system.
We can extend the system to detect when a face doesn't match anything currently in the database, and then add this new face into the database.
We can also make the system more robust by including multiple pictures of the same face with different expressions and lighting conditions.
Although there are other approaches to facial recognition that utilize more complex techniques, the method of eigenfaces remains
a wonderfully simple and effective solution and provides another illustration of the usefulness of the singular value decomposition in a variety of applications.

\begin{comment}
What remains is some old content.

\section*{Preparing the data}

Through the course of this lab we will be building a class called \li{FacialRec} to handle everything in the facial recognition system.
In the \li{specs.py} file included with this lab there is an outline of this class along with some helper functions to help us test our system.

To populate our database, we will need some images of faces.
There are many databases available online, many of which would work with our project.
However, the algorithm which we will use is not completely robust, it requires the images to have similar lighting conditions and head positions.
Tilted head, dark lighting, wearing glasses, and different hairstyles can disrupt the algorithm.
Therefore, we need a database that contains images with consistent lighting and head posistions.
Also, to make the programming easier, we would like the images to be of the same file type and size.
The University of Essex has such a database at http://cswww.essex.ac.uk/mv/allfaces/index.html, (as well as other databases with varied head positions and lighting conditions) which we will use.

The function \li{getImages} included in the specifications file loads all the JPG files from a directory into a 3 dimensional array.
The first dimension specifices the image while the other two give the grayscale image in a 2D array.
We can also use the function \li{showImage} to display the 2D grayscale images.
This will be useful for testing the database.

\begin{problem}
Download the faces94 database from the University of Essex from http://cswww.essex.ac.uk/mv/allfaces/index.html.
Initialize a \li{FacialRec} object using the file path to the directory and have it use every 50$^{th}$ face in the database.
Also, for testing purposes, use the function \li{getImages} to put the whole database into an array of images.
Then use \li{showImage} to show the first image.
\end{problem}

Now that we have images, we need to calculate a few things before we can compute the eigenfaces from the covariance matrix.
We have already reduced the images to grayscale images in \li{getImages} to make computations easier.
Now we need the mean face of all the faces in the database. The mean face is a 2D array of each pixel average across the database.
Next we need to zero out the images using the mean face.
We will substract the mean face from each face in the database, giving the the difference faces.
Then we flatten the each 2D image into a one dimensional vector so that we have each image described in single vector space.

\begin{problem}
Implement the functions \li{initMeanImage} and \li{initDifferences}.
\li{initMeanImage} should store in \li{self.meanImage} the image of the means across each pixel in the entire database.
\li{initDifferences} should first initilize \li{self.differenceFaces} to be an array of each image in the database minus the mean image.
\li{self.differenceVectors} should be the matrix with the flattened vector of each image in the database as its rows.
If the database has 75 images each of size $200 \times 180$, then \li{self.differenceVectors} should be $75 \times 36000$.
Also, plot the mean face and the first difference face.
\end{problem}


\section*{Computing the Eigenfaces with SVD}

With these pelimanaries ready, we are now able to construct the Eigenfaces of the system from the covariance matrix.
At this point we have each image descibed in a vector space with as many pixels as there are in each image.
With the database we are using, this is a 36000 dimensional vector space.
There are too many random variables to work with at this point to work with, so we need to find some way to simplify the vector space.

We do this using the eigenvectors of the covariance matrix (which we call the eigenfaces).
The eigenfaces form a basis of all possible faces, so all faces could be expressed as a linear combination of eigenfaces from the covariance matrix gathered from faces in the database.
The corresponding eigenvalues of the eigenfaces give the weight or importance of each eigenface.
Now there the covariance matrix is still very large 36000 and could have many distinct eigenvectors.
However, only eigenfaces corresponding to higher eigenvalues really make a difference.
Infact, it turns out that there are only as many eigenvalues as there are images in the database.

Recall that taking the SVD of a real valued matrix $A$ gives us the eigenvectors of the matrix $AA^T$ in the columns of $V$.
If the difference vectors are denoted by $E$, then the covariance matrix is $C = EE^T$.
It then follows that we can use the SVD of $E$ to compute the eigenvectors of $C = EE^T$.

The \li{svd} function from \li{scipy.linalg} returns $U,V,V^T$. You can then get the eigenfaces with a call like
\begin{lstlisting}
u,s,vT = la.svd(self.differenceVectors,full_matricies=False)
self.eigenfaces = vT
\end{lstlisting}
This will put into \li{eigenfaces} the eigenfaces of the database as its rows.

\begin{problem}
Implement the function \li{initEigenfaces} by computing the eigenfaces of the images in the database using the SVD and storing them in \li{self.eigenfaces}.
The eigenfaces should be in the columns of \li{self.eigenfaces} so that it becomes a $36000 \times 75$ matrix.
Plot the first three eigenfaces.
\end{problem}



With the eigenfaces computed, we can now convert vectors representing faces into the eigenface space and back.
To convert to the eigenface space we first subtract off the mean face, flatten the vector, then project the vector on the eigenfaces.
To rebuild an image from its representation in eigenfaces, we simply add up the linear combination of eigenfaces then add back in the mean.

\begin{problem}
Implement the functions \li{projectToImageFaceSpace} and \li{rebuildFromEigenfaces}.
\li{projectToImageFaceSpace} should project an image into its coordinates in the basis of the eigenfaces.
\li{rebuildFromEigenfaces} should undo that projection by taking the coordinates in the eigenface space, projecting them back, and reshaping the image.
\end{problem}



\section*{Creating the Facial Recognition System}

The facial recognition database is almost done.
Given a face to seach for, we only have to project it into the eigenface basis and then perform a nearest neighbor seach for the closest image.

\begin{problem}
Implement the functions \li{initClassifier}, and \li{findNearest}.
\li{initClassifier} should initialize \li{self.nbrs} as a \li{sklearn.neighbors.KNearestNeighbors} with \li{nearest_neighbors=1}.
Create a set of training points from the images in the database projected into the eigenface basis and use the first \li{searchOrder} number of coefficeints in the \li{self.nbrs.fit} function.
For the labels give the indicies of the labels in the database (this can be done with \li{range})

Implement \li{findNearest} by subtracting off the mean face from the image and then projecting it into the eigenface basis.
Then perform a nearest neighbor search with the coefficeints up to \li{searchOrder} using \li{self.nbrs.predict}.
This will return an index to the matching image.

We also want to compute the distance of the image to the matching image to see if it's really a good match.
Do this by projecting the matching image to the eigenface basis and then taking the norm of its difference from the image passed in but in the eigenface basis.
\end{problem}

\begin{problem}
Verification

Your facial recognition database should be working now.
Try it on some exact images that are in the database, some faces that are in the database but that aren't exact copies, and faces that aren't in the database at all.
(Pick a random image from the full set, it probably won't be in the database.
 All images that are a multiple of \li{fileStep} are in the database.
 Indicies close to these are probably of the same person but a different picture.
 As you do all of these take note of the distance between the match and the image.
 Chose a reasonable limiting distance to designate a match or false match and add whether this condition is satisfied as a boolean.
\end{problem} 
\end{comment}